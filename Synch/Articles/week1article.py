#%%
stringg = \
'''
Statistical Modeling: The Two Cultures Leo Breiman
Abstract. There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated byagivenstochasticdatamodel.Theotherusesalgorithmicmodelsand treatsthedatamechanismasunknown.Thestatisticalcommunityhas been committed to the almost exclusive use of data models. This commit- ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob- lems.Algorithmicmodeling,bothintheoryandpractice,hasdeveloped rapidlyinfieldsoutsidestatistics.Itcanbeusedbothonlargecomplex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solveproblems,thenweneedtomoveawayfromexclusivedependence on data models and adopt a more diverse set of tools.
1. INTRODUCTION
Statistics starts with data. Think of the data as beinggeneratedbyablackboxinwhichavectorof input variables x (independent variables) go in one side, and on the other side the response variables y come out. Inside the black box, nature functions to associate the predictor variables with the response variables, so the picture is like this:
yx There are two goals in analyzing the data:
Prediction. To be able to predict what the responses are going to be to future input variables; Information. To extract some information about how nature is associating the response variables to the input variables.
There are two different approaches toward these goals:
The Data Modeling Culture
The analysis in this culture starts with assuming a stochastic data model for the inside of the black box. For example, a common data model is that data aregeneratedbyindependentdrawsfrom
response variables = f(predictor variables, random noise, parameters)
Leo Breiman is Professor, Department of Statistics, University of California, Berkeley, California 94720- 4735 (e-mail: leo@stat.berkeley.edu).
The values of the parameters are estimated from the data and the model then used for information and/or prediction. Thus the black box is filled in like this:
yx
Model validation. Yes–no using goodness-of-fit tests and residual examination.
Estimated culture population. 98% of all statisti- cians.
The Algorithmic Modeling Culture
The analysis in this culture considers the inside of the box complex and unknown. Their approach is to find a function fx—an algorithm that operates on x to predict the responses y. Their black box looks like this:
yx decision trees
neural nets
Modelvalidation.Measuredbypredictiveaccuracy. Estimated culture population. 2% of statisticians, manyinotherfields.
In this paper I will argue that the focus in the statisticalcommunityondatamodelshas:
•Ledtoirrelevanttheoryandquestionablesci- entific conclusions;
  nature
      199
linear regression logistic regression Cox model
unknown
200 L. BREIMAN
• Kept statisticians from using more suitable algorithmic models;
• Prevented statisticians from working on excit- ingnewproblems;
I will also review some of the interesting new developments in algorithmic modeling in machine learning and look at applications to three data sets.
2. ROAD MAP
ItmayberevealingtounderstandhowIbecamea member of the small second culture. After a seven- year stint as an academic probabilist, I resigned and went into full-time free-lance consulting. After thir- teen years of consulting I joined the Berkeley Statis- tics Department in 1980 and have been there since. Myexperiencesasaconsultantformedmyviews about algorithmic modeling. Section 3 describes two of the projects I worked on. These are given to show howmyviewsgrewfromsuchproblems.
WhenIreturnedtotheuniversityandbegan reading statistical journals, the research was dis- tant from what I had done as a consultant. All articlesbeginandendwithdatamodels.Myobser- vations about published theoretical research in statistics are in Section 4.
Data modeling has given the statistics field many successes in analyzing data and getting informa- tion about the mechanisms producing the data. But there is also misuse leading to questionable con- clusions about the underlying mechanism. This is reviewed in Section 5. Following that is a discussion (Section 6) of how the commitment to data modeling has prevented statisticians from entering new sci- entific and commercial fields where the data being gathered is not suitable for analysis by data models.
In the past fifteen years, the growth in algorith- micmodelingapplicationsandmethodologyhas beenrapid.Ithasoccurredlargelyoutsidestatis- tics in a new community—often called machine learning—thatismostlyyoungcomputerscientists (Section7).Theadvances,particularlyoverthelast five years, have been startling. Three of the most important changes in perception to be learned from theseadvancesaredescribedinSections8,9,and 10, and are associated with the following names:
Rashomon:themultiplicityofgoodmodels; Occam:theconflictbetweensimplicityand accuracy;
Bellman: dimensionality—curse or blessing?
Section11istitled“InformationfromaBlack Box” and is important in showing that an algo- rithmic model can produce more and more reliable informationaboutthestructureoftherelationship
between inputs and outputs than data models. This is illustrated using two medical data sets and a geneticdataset.Aglossaryattheendofthepaper explainstermsthatnotallstatisticiansmaybe familiar with.
3. PROJECTS IN CONSULTING
As a consultant I designed and helped supervise surveys for the Environmental Protection Agency (EPA) and the state and federal court systems. Con- trolled experiments were designed for the EPA, and I analyzed traffic data for the U.S. Department of Transportation and the California Transportation Department. Most of all, I worked on a diverse set of prediction projects. Here are some examples:
Predictingnext-dayozonelevels. Usingmassspectratoidentifyhalogen-containing compounds.
Predicting the class of a ship from high altitude radar returns.
Using sonar returns to predict the class of a sub- marine.
Identityofhand-sentMorseCode. Toxicityofchemicals. On-linepredictionofthecauseofafreewaytraffic breakdown.
Speech recognition Thesourcesofdelayincriminaltrialsinstatecourt systems.
To understand the nature of these problems and the approaches taken to solve them, I give a fuller description of the first two on the list.
3.1 The Ozone Project
In the mid- to late 1960s ozone levels became a serioushealthproblemintheLosAngelesBasin. Threedifferentalertlevelswereestablished.Atthe highest, all government workers were directed not todrivetowork,childrenwerekeptoffplaygrounds andoutdoorexercisewasdiscouraged.
The major source of ozone at that time was auto- mobile tailpipe emissions. These rose into the low atmosphereandweretrappedtherebyaninversion layer. A complex chemical reaction, aided by sun- light,cookedawayandproducedozonetwotothree hours after the morning commute hours. The alert warnings were issued in the morning, but would be moreeffectiveiftheycouldbeissued12hoursin advance. In the mid-1970s, the EPA funded a large efforttoseeifozonelevelscouldbeaccuratelypre- dicted 12 hours in advance.
Commuting patterns in the Los Angeles Basin areregular,withthetotalvariationinanygiven

STATISTICAL MODELING: THE TWO CULTURES 201
daylight hour varying only a few percent from oneweekdaytoanother.Withthetotalamountof emissions about constant, the resulting ozone lev- elsdependonthemeteorologyofthepreceding days.Alargedatabasewasassembledconsist- ingoflowerandupperairmeasurementsatU.S. weatherstationsasfarawayasOregonandAri- zona,togetherwithhourlyreadingsofsurface temperature, humidity, and wind speed at the dozens of air pollution stations in the Basin and nearbyareas.
Altogether,thereweredailyandhourlyreadings of over 450 meteorological variables for a period of seven years, with corresponding hourly values of ozone and other pollutants in the Basin. Let x be the predictor vector of meteorological variables on the nth day. There are more than 450 variables in x since information several days back is included. Let y be the ozone level on the n + 1st day. Then the problem was to construct a function fx such thatforanyfuturedayandfuturepredictorvari- ables x for that day, fx is an accurate predictor of the next day’s ozone level y.
To estimate predictive accuracy, the first five years of data were used as the training set. The lasttwoyearsweresetasideasatestset.The algorithmicmodelingmethodsavailableinthepre- 1980s decades seem primitive now. In this project largelinearregressionswererun,followedbyvari- able selection. Quadratic terms in, and interactions among, the retained variables were added and vari- able selection used again to prune the equations. In the end, the project was a failure—the false alarm rate of the final predictor was too high. I have regrets that this project can’t be revisited with the tools available today.
3.2 The Chlorine Project
The EPA samples thousands of compounds a year and tries to determine their potential toxicity. In the mid-1970s, the standard procedure was to mea- sure the mass spectra of the compound and to try to determine its chemical structure from its mass spectra.
Measuring the mass spectra is fast and cheap. But
the determination of chemical structure from the
mass spectra requires a painstaking examination
byatrainedchemist.Thecostandavailabilityof
enough chemists to analyze all of the mass spectra
produceddauntedtheEPA.Manytoxiccompounds
contain halogens. So the EPA funded a project to
determine if the presence of chlorine in a compound
couldbereliablypredictedfromitsmassspectra. (d)Predictiveaccuracyontestsetsisthecrite-
Massspectraareproducedbybombardingthe rionforhowgoodthemodelis.
compound with ions in the presence of a magnetic (e) Computers are an indispensable partner.
field. The molecules of the compound split and the lighterfragmentsarebentmorebythemagnetic field than the heavier. Then the fragments hit an absorbing strip, with the position of the fragment on thestripdeterminedbythemolecularweightofthe fragment.Theintensityoftheexposureatthatposi- tionmeasuresthefrequencyofthefragment.The resultantmassspectrahasnumbersreflectingfre- quencies of fragments from molecular weight 1 up to the molecular weight of the original compound. The peaks correspond to frequent fragments and there aremanyzeroes.Theavailabledatabaseconsisted of the known chemical structure and mass spectra of 30,000 compounds.
The mass spectrum predictor vector x is of vari- able dimensionality. Molecular weight in the data base varied from 30 to over 10,000. The variable to be predicted is
y = 1: contains chlorine,
y = 2: does not contain chlorine.
The problem is to construct a function fx that is an accurate predictor of y where x is the mass spectrum of the compound.
Tomeasurepredictiveaccuracythedatasetwas randomlydividedintoa25,000membertraining set and a 5,000 member test set. Linear discrim- inantanalysiswastried,thenquadraticdiscrimi- nant analysis. These were difficult to adapt to the variable dimensionality. By this time I was thinking about decision trees. The hallmarks of chlorine in mass spectra were researched. This domain knowl- edge was incorporated into the decision tree algo- rithmbythedesignofthesetof1,500yes–noques- tions that could be applied to a mass spectra of any dimensionality. The result was a decision tree that gave95%accuracyonbothchlorinesandnonchlo- rines (see Breiman, Friedman, Olshen and Stone, 1984).
3.3 Perceptions on Statistical Analysis
As I left consulting to go back to the university, these were the perceptions I had about working with data to find answers to problems:
(a) Focus on finding a good solution—that’s what consultantsgetpaidfor.
(b) Live with the data before you plunge into modeling.
(c) Search for a model that gives a good solution, either algorithmic or data.

202
L. BREIMAN
4. RETURN TO THE UNIVERSITY
I had one tip about what research in the uni- versitywaslike.Afriendofmine,aprominent statisticianfromtheBerkeleyStatisticsDepart- ment, visited me in Los Angeles in the late 1970s. After I described the decision tree method to him, his first question was, “What’s the model for the data?”
4.1 Statistical Research
Uponmyreturn,IstartedreadingtheAnnalsof Statistics, the flagship journal of theoretical statis- tics,andwasbemused.Everyarticlestartedwith
Assumethatthedataaregeneratedbythefollow- ing model:   
followedbymathematicsexploringinference,hypo- thesis testing and asymptotics. There is a wide spectrum of opinion regarding the usefulness of the theorypublishedintheAnnalsofStatisticstothe field of statistics as a science that deals with data. I amattheverylowendofthespectrum.Still,there have been some gems that have combined nice theoryandsignificantapplications.Anexampleis wavelet theory. Even in applications, data models are universal. For instance, in the Journal of the American Statistical Association JASA, virtually everyarticlecontainsastatementoftheform:
Assumethatthedataaregeneratedbythefollow- ing model:   
Iamdeeplytroubledbythecurrentandpastuse of data models in applications, where quantitative conclusionsaredrawnandperhapspolicydecisions made.
5. THE USE OF DATA MODELS
Statisticians in applied research consider data modeling as the template for statistical analysis: Faced with an applied problem, think of a data model. This enterprise has at its heart the belief thatastatistician,byimaginationandbylooking atthedata,caninventareasonablygoodpara- metric class of models for a complex mechanism devisedbynature.Thenparametersareestimated and conclusions are drawn. But when a model is fit to data to draw quantitative conclusions:
• The conclusions are about the model’s mecha- nism, and not about nature’s mechanism.
It follows that:
• If the model is a poor emulation of nature, the conclusionsmaybewrong.
These truisms have often been ignored in the enthu- siasm for fitting data models. A few decades ago, the commitment to data models was such that even simple precautions such as residual analysis or goodness-of-fit tests were not used. The belief in the infallibilityofdatamodelswasalmostreligious.It is a strange phenomenon—once a model is made, then it becomes truth and the conclusions from it are infallible.
5.1 An Example
I illustrate with a famous (also infamous) exam- ple:assumethedataisgeneratedbyindependent draws from the model
M
R y=b0+ bmxm+ε
1
wherethecoefficientsbmaretobeestimated,ε is N0 σ2 and σ2 is to be estimated. Given that the data is generated this way, elegant tests of hypotheses,confidenceintervals,distributionsof the residual sum-of-squares and asymptotics can be derived. This made the model attractive in terms ofthemathematicsinvolved.Thistheorywasused bothbyacademicstatisticiansandotherstoderive significance levels for coefficients on the basis of model (R), with little consideration as to whether thedataonhandcouldhavebeengeneratedbya linear model. Hundreds, perhaps thousands of arti- cles were published claiming proof of something or other because the coefficient was significant at the 5% level.
Goodness-of-fitwasdemonstratedmostlybygiv- ing the value of the multiple correlation coefficient R2whichwasoftenclosertozerothanoneand whichcouldbeoverinflatedbytheuseoftoomany parameters. Besides computing R2, nothing else was done to see if the observational data could have beengeneratedbymodel(R).Forinstance,astudy wasdoneseveraldecadesagobyawell-known memberofauniversitystatisticsdepartmentto assess whether there was gender discrimination in the salaries of the faculty. All personnel files were examined and a data base set up which consisted of salaryastheresponsevariableand25othervari- ables which characterized academic performance; thatis,paperspublished,qualityofjournalspub- lished in, teaching record, evaluations, etc. Gender appearsasabinarypredictorvariable.
A linear regression was carried out on the data and the gender coefficient was significant at the 5% level. That this was strong evidence of sex dis- crimination was accepted as gospel. The design ofthestudyraisesissuesthatenterbeforethe considerationofamodel—Canthedatagathered

answerthequestionposed?Isinferencejustified when your sample is the entire population? Should a data model be used? The deficiencies in analysis occurred because the focus was on the model and not on the problem.
Thelinearregressionmodelledtomanyerro- neous conclusions that appeared in journal articles waving the 5% significance level without knowing whether the model fit the data. Nowadays, I think most statisticians will agree that this is a suspect waytoarriveatconclusions.Atthetime,therewere few objections from the statistical profession about the fairy-tale aspect of the procedure, But, hidden in anelementarytextbook,MostellerandTukey(1977) discussmanyofthefallaciespossibleinregression and write “The whole area of guided regression is fraught with intellectual, statistical, computational, and subject matter difficulties.”
Even currently, there are only rare published cri- tiques of the uncritical use of data models. One of the few is David Freedman, who examines the use of regression models (1994); the use of path models (1987) and data modeling (1991, 1995). The analysis in these papers is incisive.
5.2 Problems in Current Data Modeling
Current applied practice is to check the data model fit using goodness-of-fit tests and residual analysis. At one point, some years ago, I set up a simulated regression problem in seven dimensions with a controlled amount of nonlinearity. Standard testsofgoodness-of-fitdidnotrejectlinearityuntil thenonlinearitywasextreme.Recenttheorysup- portsthisconclusion.WorkbyBickel,Ritovand Stoker (2001) shows that goodness-of-fit tests have verylittlepowerunlessthedirectionofthealter- nativeispreciselyspecified.Theimplicationisthat omnibus goodness-of-fit tests, which test in many directions simultaneously, have little power, and will not reject until the lack of fit is extreme.
Furthermore, if the model is tinkered with on the basis of the data, that is, if variables are deleted or nonlinear combinations of the variables added, then goodness-of-fit tests are not applicable. Resid- ual analysis is similarly unreliable. In a discussion after a presentation of residual analysis in a sem- inaratBerkeleyin1993,WilliamCleveland,one ofthefathersofresidualanalysis,admittedthatit could not uncover lack of fit in more than four to five dimensions. The papers I have read on using resid- ualanalysistochecklackoffitareconfinedtodata sets with two or three variables.
With higher dimensions, the interactions between the variables can produce passable residual plots for
avarietyofmodels.Aresidualplotisagoodness-of- fit test, and lacks power in more than a few dimen- sions. An acceptable residual plot does not imply that the model is a good fit to the data.
There are a variety of ways of analyzing residuals. Forinstance,Landwher,PreibonandShoemaker (1984, with discussion) gives a detailed analysis of fitting a logistic model to a three-variable data set using various residual plots. But each of the four discussants present other methods for the analysis. One is left with an unsettled sense about the arbi- trariness of residual analysis.
Misleadingconclusionsmayfollowfromdata models that pass goodness-of-fit tests and residual checks. But published applications to data often show little care in checking model fit using these methodsoranyother.Forinstance,manyofthe current application articles in JASA that fit data modelshaveverylittlediscussionofhowwelltheir model fits the data. The question of how well the modelfitsthedataisofsecondaryimportancecom- pared to the construction of an ingenious stochastic model.
5.3 The Multiplicity of Data Models
One goal of statistics is to extract information from the data about the underlying mechanism pro- ducing the data. The greatest plus of data modeling is that it produces a simple and understandable pic- ture of the relationship between the input variables and responses. For instance, logistic regression in classificationisfrequentlyusedbecauseitproduces a linear combination of the variables with weights that give an indication of the variable importance. The end result is a simple picture of how the pre- diction variables affect the response variable plus confidence intervals for the weights. Suppose two statisticians, each one with a different approach to data modeling, fit a model to the same data set. Assume also that each one applies standard goodness-of-fit tests, looks at residuals, etc., and is convinced that their model fits the data. Yet the two models give different pictures of nature’s mechanism and lead to different conclusions.
McCullah and Nelder (1989) write “Data will often point with almost equal emphasis on sev- eral possible models, and it is important that the statistician recognize and accept this.” Well said, butdifferentmodels,allofthemequallygood,may give different pictures of the relation between the predictor and response variables. The question of whichonemostaccuratelyreflectsthedataisdif- ficult to resolve. One reason for this multiplicity is that goodness-of-fit tests and other methods for checking fit give a yes–no answer. With the lack of
STATISTICAL MODELING: THE TWO CULTURES 203

204 L. BREIMAN
powerofthesetestswithdatahavingmorethana smallnumberofdimensions,therewillbealarge numberofmodelswhosefitisacceptable.Thereis no way, among the yes–no methods for gauging fit, ofdeterminingwhichisthebettermodel.Afew statisticiansknowthis.MountainandHsiao(1989) write,“Itisdifficulttoformulateacomprehensive model capable of encompassing all rival models. Furthermore, with the use of finite samples, there aredubiousimplicationswithregardtothevalidity and power of various encompassing tests that rely on asymptotic theory.”
Datamodelsincurrentusemayhavemoredam- aging results than the publications in the social sci- ences based on a linear regression analysis. Just as the 5% level of significance became a de facto stan- dard for publication, the Cox model for the analysis of survival times and logistic regression for survive– nonsurvive data have become the de facto standard for publication in medical journals. That different survivalmodels,equallywellfitting,couldgivedif- ferent conclusions is not an issue.
5.4 Predictive Accuracy
Themostobviouswaytoseehowwellthemodel box emulates nature’s box is this: put a case x down nature’s box getting an output y. Similarly, put the same case x down the model box getting an out- put y′. The closeness of y and y′ is a measure of how good the emulation is. For a data model, this translates as: fit the parameters in your model by using the data, then, using the model, predict the data and see how good the prediction is.
Predictionisrarelyperfect.Thereareusu- allymanyunmeasuredvariableswhoseeffectis referred to as “noise.” But the extent to which the model box emulates nature’s box is a measure of how well our model can reproduce the natural phenomenon producing the data.
McCullagh and Nelder (1989) in their book on generalized linear models also think the answer is obvious.Theywrite,“Atfirstsightitmightseem as though a good model is one that fits the data verywell;thatis,onethatmakesμˆ(themodelpre- dictedvalue)veryclosetoy(theresponsevalue).” Thentheygoontonotethattheextentoftheagree- mentisbiasedbythenumberofparametersused inthemodelandsoisnotasatisfactorymeasure. Theyare,ofcourse,right.Ifthemodelhastoomany parameters,thenitmayoverfitthedataandgivea biased estimate of accuracy. But there are ways to remove the bias. To get a more unbiased estimate of predictive accuracy, cross-validation can be used, asadvocatedinanimportantearlyworkbyStone (1974). If the data set is larger, put aside a test set.
MostellerandTukey(1977)wereearlyadvocates ofcross-validation.Theywrite,“Cross-validationis anaturalroutetotheindicationofthequalityofany data-derived quantity   . We plan to cross-validate carefullywhereverwecan.”
Judgingbytheinfrequencyofestimatesofpre- dictiveaccuracyinJASA,thismeasureofmodel fit that seems natural to me (and to Mosteller and Tukey) is not natural to others. More publication of predictiveaccuracyestimateswouldestablishstan- dards for comparison of models, a practice that is common in machine learning.
6. THE LIMITATIONS OF DATA MODELS
With the insistence on data models, multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification and multiple linear regression in regression. Nobody reallybelievesthatmultivariatedataismultivari- ate normal, but that data model occupies a large numberofpagesineverygraduatetextbookon multivariate statistical analysis.
With data gathered from uncontrolled observa- tions on complex systems involving unknown physi- cal, chemical, or biological mechanisms, the a priori assumption that nature would generate the data throughaparametricmodelselectedbythestatis- tician can result in questionable conclusions that cannotbesubstantiatedbyappealtogoodness-of-fit tests and residual analysis. Usually, simple para- metricmodelsimposedondatageneratedbycom- plex systems, for example, medical data, financial data,resultinalossofaccuracyandinformationas compared to algorithmic models (see Section 11).
There is an old saying “If all a man has is a hammer,theneveryproblemlookslikeanail.”The troubleforstatisticiansisthatrecentlysomeofthe problems have stopped looking like nails. I conjec- ture that the result of hitting this wall is that more complicated data models are appearing in current published applications. Bayesian methods combined with Markov Chain Monte Carlo are cropping up all over.Thismaysignifythatasdatabecomesmore complex,thedatamodelsbecomemorecumbersome andarelosingtheadvantageofpresentingasimple andclearpictureofnature’smechanism.
Approachingproblemsbylookingforadatamodel imposesanaprioristraightjacketthatrestrictsthe abilityofstatisticianstodealwithawiderangeof statistical problems. The best available solution to a data problem might be a data model; then again it might be an algorithmic model. The data and the problemguidethesolution.Tosolveawiderrange of data problems, a larger set of tools is needed.

Perhaps the damaging consequence of the insis- tence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen outoftherapidlyincreasingabilityofcomputers to store and manipulate data. These problems are increasinglypresentinmanyfields,bothscientific and commercial, and solutions are being found by nonstatisticians.
7. ALGORITHMIC MODELING
Under other names, algorithmic modeling has beenusedbyindustrialstatisticiansfordecades. See, for instance, the delightful book “Fitting Equa- tions to Data” (Daniel and Wood, 1971). It has been usedbypsychometriciansandsocialscientists. ReadingapreprintofGifi’sbook(1990)manyyears ago uncovered a kindred spirit. It has made small inroads into the analysis of medical data starting withRichardOlshen’sworkintheearly1980s.For further work, see Zhang and Singer (1999). Jerome Friedman and Grace Wahba have done pioneering work on the development of algorithmic methods. But the list of statisticians in the algorithmic mod- eling business is short, and applications to data are seldom seen in the journals. The development of algorithmicmethodswastakenupbyacommunity outside statistics.
7.1ANewResearchCommunity
In the mid-1980s two powerful new algorithms for fitting data became available: neural nets and decisiontrees.Anewresearchcommunityusing these tools sprang up. Their goal was predictive accuracy. The community consisted of young com- puter scientists, physicists and engineers plus a few agingstatisticians.Theybeganusingthenewtools in working on complex prediction problems where it was obvious that data models were not applicable: speech recognition, image recognition, nonlinear time series prediction, handwriting recognition, prediction in financial markets.
Theirinterestsrangeovermanyfieldsthatwere onceconsideredhappyhuntinggroundsforstatisti- cians and have turned out thousands of interesting research papers related to applications and method- ology. A large majority of the papers analyze real data.Thecriterionforanymodeliswhatisthepre- dictiveaccuracy.Anideaoftherangeofresearch ofthisgroupcanbegotbylookingattheProceed- ings of the Neural Information Processing Systems Conference (their main yearly meeting) or at the Machine Learning Journal.
7.2 Theory in Algorithmic Modeling
Datamodelsarerarelyusedinthiscommunity. The approach is that nature produces data in a black box whose insides are complex, mysterious, and,atleast,partlyunknowable.Whatisobserved is a set of x’s that go in and a subsequent set of y’s that come out. The problem is to find an algorithm fx such that for future x in a test set, fx will be a good predictor of y.
Thetheoryinthisfieldshiftsfocusfromdatamod- els to the properties of algorithms. It characterizes their “strength” as predictors, convergence if they are iterative, and what gives them good predictive accuracy. The one assumption made in the theory is that the data is drawn i.i.d. from an unknown multivariate distribution.
There is isolated work in statistics where the focusisonthetheoryofthealgorithms.Grace Wahba’s research on smoothing spline algo- rithms and their applications to data (using cross- validation)isbuiltontheoryinvolvingreproducing kernels in Hilbert Space (1990). The final chapter of the CART book (Breiman et al., 1984) contains a proof of the asymptotic convergence of the CART algorithm to the Bayes risk by letting the trees grow as the sample size increases. There are others, but therelativefrequencyissmall.
Theoryresultedinamajoradvanceinmachine learning. Vladimir Vapnik constructed informative boundsonthegeneralizationerror(infinitetestset error) of classification algorithms which depend on the “capacity” of the algorithm. These theoretical bounds led to support vector machines (see Vapnik, 1995, 1998) which have proved to be more accu- rate predictors in classification and regression then neural nets, and are the subject of heated current research (see Section 10).
Mylastpaper“Someinfinitytheoryfortree ensembles” (Breiman, 2000) uses a function space analysis to try and understand the workings of tree ensemble methods. One section has the heading, “Mykingdomforsomegoodtheory.”Thereisan effective method for forming ensembles known as “boosting,”butthereisn’tanyfinitesamplesize theorythattellsuswhyitworkssowell.
7.3 Recent Lessons
Theadvancesinmethodologyandincreasesin predictiveaccuracysincethemid-1980sthathave occurredintheresearchofmachinelearninghas been phenomenal. There have been particularly exciting developments in the last five years. What has been learned? The three lessons that seem most
STATISTICAL MODELING: THE TWO CULTURES 205

206 L. BREIMAN
important to one:
Rashomon:themultiplicityofgoodmodels; Occam:theconflictbetweensimplicityandaccu- racy;
Bellman: dimensionality—curse or blessing.
8. RASHOMON AND THE MULTIPLICITY OF GOOD MODELS
neural net 100 times on simple three-dimensional data reselecting the initial weights to be small and random on each run. I found 32 distinct minima, each of which gave a different picture, and having about equal test set error.
ThiseffectiscloselyconnectedtowhatIcall instability(Breiman,1996a)thatoccurswhenthere aremanydifferentmodelscrowdedtogetherthat have about the same training or test set error. Then a slight perturbation of the data or in the model construction will cause a skip from one model to another. The two models are close to each other in termsoferror,butcanbedistantintermsofthe formofthemodel.
Rashomon is a wonderful Japanese movie in
which four people, from different vantage points,
witness an incident in which one person dies and
anotherissupposedlyraped.Whentheycometo
testifyincourt,theyallreportthesamefacts,but
theirstoriesofwhathappenedareverydifferent. If,inlogisticregressionortheCoxmodel,the
What I call the Rashomon Effect is that there is often a multitude of different descriptions [equa- tions fx] in a class of functions giving about the sameminimumerrorrate.Themosteasilyunder- stood example is subset selection in linear regres- sion. Suppose there are 30 variables and we want to find the best five variable linear regressions. There are about 140,000 five-variable subsets in competi- tion.Usuallywepicktheonewiththelowestresid- ual sum-of-squares (RSS), or, if there is a test set, thelowesttesterror.Buttheremaybe(andgen- erallyare)manyfive-variableequationsthathave RSS within 1.0% of the lowest RSS (see Breiman, 1996a). The same is true if test set error is being measured.
So here are three possible pictures with RSS or test set error within 1.0% of each other:
Picture 1 y=21+38x3−06x8+832x12
− 21x17 + 32x27
y = −89 + 46x5 + 001x6 + 120x15
+ 175x21 + 02x22
Picture 3
y= −767 + 93x2 + 220x7 − 132x8
+ 34x11 + 72x28
Whichoneisbetter?Theproblemisthateachone tellsadifferentstoryaboutwhichvariablesare important.
TheRashomonEffectalsooccurswithdecision treesandneuralnets.Inmyexperimentswithtrees, ifthetrainingsetisperturbedonlyslightly,sayby removingarandom2–3%ofthedata,Icanget a tree quite different from the original but with almost the same test set error. I once ran a small
common practice of deleting the less important covariates is carried out, then the model becomes unstable—therearetoomanycompetingmodels. Sayyouaredeletingfrom15variablesto4vari- ables.Perturbthedataslightlyandyouwillvery possiblygetadifferentfour-variablemodeland a different conclusion about which variables are important.Toimproveaccuracybyweedingoutless important covariates you run into the multiplicity problem. The picture of which covariates are impor- tantcanvarysignificantlybetweentwomodels having about the same deviance.
Aggregating over a large set of competing mod- els can reduce the nonuniqueness while improving accuracy. Arena et al. (2000) bagged (see Glossary) logistic regression models on a data base of toxic and nontoxic chemicals where the number of covariates ineachmodelwasreducedfrom15to4bystan- dard best subset selection. On a test set, the bagged modelwassignificantlymoreaccuratethanthesin- gle model with four covariates. It is also more stable. Thisisonepossiblefix.Themultiplicityproblem and its effect on conclusions drawn from models needs serious attention.
9. OCCAM AND SIMPLICITY VS. ACCURACY
Occam’sRazor,longadmired,isusuallyinter- preted to mean that simpler is better. Unfortunately, inprediction,accuracyandsimplicity(interpretabil- ity) are in conflict. For instance, linear regression givesafairlyinterpretablepictureoftheyxrela- tion.Butitsaccuracyisusuallylessthanthat of the less interpretable neural nets. An example closertomyworkinvolvestrees.
Oninterpretability,treesrateanA+.Aproject Iworkedoninthelate1970swastheanalysisof delayincriminalcasesinstatecourtsystems.The Constitution gives the accused the right to a speedy trial. The Center for the State Courts was concerned
Picture 2

thatinmanystates,thetrialswereanythingbut speedy. It funded a study of the causes of the delay. Ivisitedmanystatesanddecidedtodotheanal- ysis in Colorado, which had an excellent computer- ized court data system. A wealth of information was extracted and processed.
The dependent variable for each criminal case was the time from arraignment to the time of sen- tencing. All of the other information in the trial his- torywerethepredictorvariables.Alargedecision tree was grown, and I showed it on an overhead and explained it to the assembled Colorado judges. One of the splits was on District N which had a larger delaytimethantheotherdistricts.Irefrainedfrom commenting on this. But as I walked out I heard one judgesaytoanother,“IknewthoseguysinDistrict N were dragging their feet.”
While trees rate an A+ on interpretability, they are good, but not great, predictors. Give them, say, a B on prediction.
9.1 Growing Forests for Prediction
Instead of a single tree predictor, grow a forest of treesonthesamedata—say50or100.Ifweare classifying, put the new x down each tree in the for- est and get a vote for the predicted class. Let the for- est prediction be the class that gets the most votes. There has been a lot of work in the last five years on ways to grow the forest. All of the well-known meth- odsgrowtheforestbyperturbingthetrainingset, growing a tree on the perturbed training set, per- turbing the training set again, growing another tree, etc. Some familiar methods are bagging (Breiman, 1996b), boosting (Freund and Schapire, 1996), arc- ing (Breiman, 1998), and additive logistic regression (Friedman, Hastie and Tibshirani, 1998).
Mypreferredmethodtodateisrandomforests.In this approach successive decision trees are grown by introducing a random element into their construc- tion. For example, suppose there are 20 predictor
variables.Ateachnodechooseseveralofthe20at random to use to split the node. Or use a random combinationofarandomselectionofafewvari- ables. This idea appears in Ho (1998), in Amit and Geman (1997) and is developed in Breiman (1999).
9.2 Forests Compared to Trees
We compare the performance of single trees (CART) to random forests on a number of small andlargedatasets,mostlyfromtheUCIrepository (ftp.ics.uci.edu/pub/MachineLearningDatabases). A summaryofthedatasetsisgiveninTable1.
Table 2 compares the test set error of a single tree to that of the forest. For the five smaller data sets above the line, the test set error was estimated by leaving out a random 10% of the data, then run- ning CART and the forest on the other 90%. The left-out 10% was run down the tree and the forest and the error on this 10% computed for both. This was repeated 100 times and the errors averaged. The larger data sets below the line came with a separate test set. People who have been in the clas- sification field for a while find these increases in accuracystartling.Someerrorsarehalved.Others arereducedbyone-third.Inregression,wherethe
STATISTICAL MODELING: THE TWO CULTURES
207
Training Data set Sample size
Cancer 699 Ionosphere 351 Diabetes 768 Glass 214 Soybean 683
Letters 15,000 Satellite 4,435 Shuttle 43,500 DNA 2,000 Digit 7,291
Test Sample size
— — — — —
5000
2000 14,500 1,186 2,007
Variables
9 34 8 9 35
16 36 9 60 256
Classes
2 2 2 6
19
26 6 7 3 10
Table 1
Data set descriptions
   Table 2
Test set misclassification error (%)
Dataset Forest
Singletree
  Breast cancer 2.9 5.9 Ionosphere 5.5 11.2 Diabetes 24.2 25.3 Glass 22.0 30.4
Soybean
Letters Satellite Shuttle ×103 DNA
Digit
5.7 8.6
3.4 12.4 8.6 14.8 7.0 62.0 3.9 6.2 6.2 17.1
 
208 L. BREIMAN
forest prediction is the average over the individual treepredictions,thedecreasesinmean-squaredtest set error are similar.
9.3 Random Forests are A + Predictors
The Statlog Project (Mitchie, Spiegelhalter and Taylor, 1994) compared 18 different classifiers. Included were neural nets, CART, linear and quadratic discriminant analysis, nearest neighbor, etc. The first four data sets below the line in Table 1 weretheonlyonesusedintheStatlogProjectthat came with separate test sets. In terms of rank of accuracyonthesefourdatasets,theforestcomes in 1, 1, 1, 1 for an average rank of 1.0. The next bestclassifierhadanaveragerankof7.3.
The fifth data set below the line consists of 16×16 pixelgrayscaledepictionsofhandwrittenZIPCode numerals.IthasbeenextensivelyusedbyAT&T BellLabstotestavarietyofpredictionmethods. A neural net handcrafted to the data got a test set error of 5.1% vs. 6.2% for a standard run of random forest.
9.4 The Occam Dilemma
So forests are A+ predictors. But their mechanism for producing a prediction is difficult to understand. Tryingtodelveintothetangledwebthatgenerated apluralityvotefrom100treesisaHerculeantask. So on interpretability, they rate an F. Which brings ustotheOccamdilemma:
•Accuracygenerallyrequiresmorecomplexpre- diction methods. Simple and interpretable functions do not make the most accurate predictors.
Usingcomplexpredictorsmaybeunpleasant,but the soundest path is to go for predictive accuracy first,thentrytounderstandwhy.Infact,Section 10 points out that from a goal-oriented statistical viewpoint, there is no Occam’s dilemma. (For more on Occam’s Razor see Domingos, 1998, 1999.)
10.BELLMANANDTHECURSEOF DIMENSIONALITY
The title of this section refers to Richard Bell- man’s famous phrase, “the curse of dimensionality.” For decades, the first step in prediction methodol- ogywastoavoidthecurse.Ifthereweretoomany prediction variables, the recipe was to find a few features (functions of the predictor variables) that “contain most of the information” and then use these features to replace the original variables. In procedures common in statistics such as regres- sion, logistic regression and survival models the advised practice is to use variable deletion to reduce
the dimensionality. The published advice was that highdimensionalityisdangerous.Forinstance,a well-regarded book on pattern recognition (Meisel, 1972) states “the features must be relatively few in number.” But recent work has shown that dimensionalitycanbeablessing.
10.1 Digging It Out in Small Pieces
Reducingdimensionalityreducestheamountof information available for prediction. The more pre- dictor variables, the more information. There is also information in various combinations of the predictor variables.Let’strygoingintheoppositedirection:
• Instead of reducing dimensionality, increase it byaddingmanyfunctionsofthepredictorvariables.
Theremaynowbethousandsoffeatures.Each potentiallycontainsasmallamountofinformation. The problem is how to extract and put together these little pieces of information. There are two outstanding examples of work in this direction, The Shape Recognition Forest (Y. Amit and D. Geman, 1997) and Support Vector Machines (V. Vapnik, 1995, 1998).
10.2 The Shape Recognition Forest
In 1992, the National Institute of Standards and Technology(NIST)setupacompetitionformachine algorithmstoreadhandwrittennumerals.Theyput together a large set of pixel pictures of handwritten numbers(223,000)writtenbyover2,000individ- uals. The competition attracted wide interest, and diverse approaches were tried.
TheAmit–Gemanapproachdefinedmanythou- sands of small geometric features in a hierarchi- cal assembly. Shallow trees are grown, such that at each node, 100 features are chosen at random from the appropriate level of the hierarchy; and the opti- mal split of the node based on the selected features is found.
When a pixel picture of a number is dropped down a single tree, the terminal node it lands in gives probabilityestimatesp0p9thatitrepresents numbers 0 1    9. Over 1,000 trees are grown, the probabilities averaged over this forest, and the pre- dicted number is assigned to the largest averaged probability.
Using a 100,000 example training set and a 50,000 test set, the Amit–Geman method gives a test set error of 0.7%–close to the limits of human error.
10.3 Support Vector Machines
Suppose there is two-class data having prediction vectors in M-dimensional Euclidean space. The pre- diction vectors for class #1 are x1 and those for

class #2 are x2. If these two sets of vectors can beseparatedbyahyperplanethenthereisanopti- malseparatinghyperplane.“Optimal”isdefinedas meaning that the distance of the hyperplane to any predictionvectorismaximal(seebelow).
Thesetofvectorsinx1andinx2that achieve the minimum distance to the optimal separating hyperplane are called the support vec- tors. Their coordinates determine the equation of the hyperplane. Vapnik (1995) showed that if a separating hyperplane exists, then the optimal sep- aratinghyperplanehaslowgeneralizationerror (see Glossary).
optimal hyperplane support vector
Intwo-classdata,separabilitybyahyperplane does not often occur. However, let us increase the dimensionalitybyaddingasadditionalpredictor variables all quadratic monomials in the original predictor variables; that is, all terms of the form xm1xm2. A hyperplane in the original variables plus quadratic monomials in the original variables is a morecomplexcreature.Thepossibilityofsepara- tion is greater. If no separation occurs, add cubic monomials as input features. If there are originally 30 predictor variables, then there are about 40,000 features if monomials up to the fourth degree are added.
Thehigherthedimensionalityofthesetoffea- tures,themorelikelyitisthatseparationoccurs.In the ZIP Code data set, separation occurs with fourth degree monomials added. The test set error is 4.1%. Using a large subset of the NIST data base as a training set, separation also occurred after adding up to fourth degree monomials and gave a test set error rate of 1.1%.
Separation can always be had by raising the dimensionalityhighenough.Butiftheseparating hyperplanebecomestoocomplex,thegeneralization error becomes large. An elegant theorem (Vapnik, 1995)givesthisboundfortheexpectedgeneraliza- tionerror:
ExGE ≤ Exnumber of support vectors/N − 1
whereNisthesamplesizeandtheexpectationis over all training sets of size N drawn from the same underlying distribution as the original training set.
Thenumberofsupportvectorsincreaseswiththe dimensionalityofthefeaturespace.Ifthisnumber
becomes too large, the separating hyperplane will notgivelowgeneralizationerror.Ifseparationcan- notberealizedwitharelativelysmallnumberof support vectors, there is another version of support vectormachinesthatdefinesoptimalitybyadding apenaltytermforthevectorsonthewrongsideof the hyperplane.
Some ingenious algorithms make finding the opti- mal separating hyperplane computationally feasi- ble. These devices reduce the search to a solution of a quadratic programming problem with linear inequalityconstraintsthatareoftheorderofthe number N of cases, independent of the dimension of the feature space. Methods tailored to this partic- ular problem produce speed-ups of an order of mag- nitude over standard methods for solving quadratic programming problems.
Support vector machines can also be used to provide accurate predictions in other areas (e.g., regression). It is an exciting idea that gives excel- lent performance and is beginning to supplant the use of neural nets. A readable introduction is in Cristianini and Shawe-Taylor (2000).
11. INFORMATION FROM A BLACK BOX
The dilemma posed in the last section is that the models that best emulate nature in terms of predictiveaccuracyarealsothemostcomplexand inscrutable. But this dilemma can be resolved by realizing the wrong question is being asked. Nature formstheoutputsyfromtheinputsxbymeansof a black box with complex and unknown interior.
yx
Current accurate prediction methods are also
complex black boxes.
yx
So we are facing two black boxes, where ours seemsonlyslightlylessinscrutablethannature’s. Indatageneratedbymedicalexperiments,ensem- bles of predictors can give cross-validated error ratessignificantlylowerthanlogisticregression. Mybiostatisticianfriendstellme,“Doctorscan interpretlogisticregression.”Thereisnowaythey caninterpretablackboxcontainingfiftytrees hookedtogether.Inachoicebetweenaccuracyand interpretability, they’ll go for interpretability.
Framing the question as the choice between accu- racyandinterpretabilityisanincorrectinterpre- tationofwhatthegoalofastatisticalanalysisis.
STATISTICAL MODELING: THE TWO CULTURES 209
   nature
 neural nets forests
support vectors

210 L. BREIMAN
The point of a model is to get useful information about the relation between the response and pre- dictorvariables.Interpretabilityisawayofgetting information. But a model does not have to be simple to provide reliable information about the relation between predictor and response variables; neither doesithavetobeadatamodel.
• The goal is not interpretability, but accurate information.
The following three examples illustrate this point. The first shows that random forests applied to a medical data set can give more reliable informa- tion about covariate strengths than logistic regres- sion. The second shows that it can give interesting informationthatcouldnotberevealedbyalogistic regression. The third is an application to a microar- raydatawhereitisdifficulttoconceiveofadata model that would uncover similar information.
11.1 Example I: Variable Importance in a Survival Data Set
The data set contains survival or nonsurvival of 155 hepatitis patients with 19 covariates. It is available at ftp.ics.uci.edu/pub/MachineLearning- DatabasesandwascontributedbyGailGong.The descriptionisinafilecalledhepatitis.names.The datasethasbeenpreviouslyanalyzedbyDiaconis andEfron(1983),andCestnik,Konenenkoand Bratko(1987).Thelowestreportederrorrateto date, 17%, is in the latter paper.
DiaconisandEfronrefertoworkbyPeterGre- goryoftheStanfordMedicalSchoolwhoanalyzed this data and concluded that the important vari- ableswerenumbers6,12,14,19andreportsanesti- mated 20% predictive accuracy. The variables were reducedintwostages—thefirstwasbyinformal data analysis. The second refers to a more formal
3.5
2.5
1.5
.5
(unspecified) statistical procedure which I assume was logistic regression.
EfronandDiaconisdrew500bootstrapsamples from the original data set and used a similar pro- cedure to isolate the important variables in each bootstrapped data set. The authors comment, “Of thefourvariablesoriginallyselectednotonewas selected in more than 60 percent of the samples. Hence the variables identified in the original analy- sis cannot be taken too seriously.” We will come back to this conclusion later.
Logistic Regression
The predictive error rate for logistic regression on the hepatitis data set is 17.4%. This was evaluated bydoing100runs,eachtimeleavingoutarandomly selected 10% of the data as a test set, and then averaging over the test set errors.
Usually, the initial evaluation of which variables are important is based on examining the absolute values of the coefficients of the variables in the logis- ticregressiondividedbytheirstandarddeviations. Figure 1 is a plot of these values.
The conclusion from looking at the standard- ized coefficients is that variables 7 and 11 are the mostimportantcovariates.Whenlogisticregres- sionisrunusingonlythesetwovariables,the cross-validatederrorraterisesto22.9%.Another waytofindimportantvariablesistorunabest subsetssearchwhich,foranyvaluek,findsthe subset of k variables having lowest deviance.
Thisprocedureraisestheproblemsofinstability andmultiplicityofmodels(seeSection7.1).There are about 4,000 subsets containing four variables. Ofthese,therearealmostcertainlyasubstantial number that have deviance close to the minimum andgivedifferentpicturesofwhattheunderlying mechanism is.
–.5
0 1 2 3 4 5 6 7 8 9 1011121314151617181920
variables
Fig. 1. Standardized coefficients logistic regression.
      standardized coefficients
 
0 –1 –2 –3
0
–1
–2
STATISTICAL MODELING: THE TWO CULTURES 211
50 40 30 20 10
0
–10
0 1 2 3 4 5 6 7 8 9 1011121314151617181920
variables
Fig. 2. Variable importance-random forest.
Random Forests
The random forests predictive error rate, evalu- atedbyaveragingerrorsover100runs,eachtime leaving out 10% of the data as a test set, is 12.3%— almost a 30% reduction from the logistic regression error.
Random forests consists of a large number of randomlyconstructedtrees,eachvotingforaclass. Similar to bagging (Breiman, 1996), a bootstrap sample of the training set is used to construct each tree. A random selection of the input variables is searched to find the best split for each node.
To measure the importance of the mth variable, thevaluesofthemthvariablearerandomlyper- muted in all of the cases left out in the current bootstrap sample. Then these cases are run down thecurrenttreeandtheirclassificationnoted.At theendofarunconsistingofgrowingmanytrees, the percent increase in misclassification rate due to noising up each variable is computed. This is the
measure of variable importance that is shown in Figure 1.
Random forests singles out two variables, the 12th and the 17th, as being important. As a verifi- cation both variables were run in random forests, individuallyandtogether.Thetestseterrorrates over 100 replications were 14.3% each. Running both together did no better. We conclude that virtu- allyallofthepredictivecapabilityisprovidedbya single variable, either 12 or 17.
To explore the interaction between 12 and 17 a bit further, at the end of a random forest run using all variables, the output includes the estimated value oftheprobabilityofeachclassvs.thecasenumber. This information is used to get plots of the vari- able values (normalized to mean zero and standard deviationone)vs.theprobabilityofdeath.Thevari- ablevaluesaresmoothedusingaweightedlinear regression smoother. The results are in Figure 3 for variables 12 and 17.
VARIABLE 12 vs PROBABILITY #1 11
VARIABLE 17 vs PROBABILITY #1
–4 –3 0 .2 .4 .6 .8 1
class 1 probability
0 .2 .4 .6 .8 1 class 1 probability
Fig. 3. Variable 17 vs. probability #1.
                   variable 12
percent increse in error
variable 17
  
212 L. BREIMAN
 Thegraphsofthevariablevaluesvs.classdeath probabilityarealmostlinearandsimilar.Thetwo variablesturnouttobehighlycorrelated.Thinking that this might have affected the logistic regression results, it was run again with one or the other of these two variables deleted. There was little change.
Out of curiosity, I evaluated variable impor- tanceinlogisticregressioninthesamewaythatI didinrandomforests,bypermutingvariableval- ues in the 10% test set and computing how much that increased the test set error. Not much help— variables 12 and 17 were not among the 3 variables ranked as most important. In partial verification of the importance of 12 and 17, I tried them sep- aratelyassinglevariablesinlogisticregression. Variable 12 gave a 15.7% error rate, variable 17 came in at 19.3%.
To go back to the original Diaconis–Efron analy- sis, the problem is clear. Variables 12 and 17 are sur- rogatesforeachother.Ifoneofthemappearsimpor- tantinamodelbuiltonabootstrapsample,the otherdoesnot.Soeachone’sfrequencyofoccurrence
isautomaticallylessthan50%.Thepaperliststhe variablesselectedintenofthesamples.Either12 or17appearinsevenoftheten.
11.2 Example II Clustering in Medical Data
The Bupa liver data set is a two-class biomedical data set also available at ftp.ics.uci.edu/pub/Mac- hineLearningDatabases. The covariates are:
Fig. 4. Variable importance—Bupa data.
1. mcv
2. alkphos
3. sgpt
4. sgot
5. gammagt
6. drinks
mean corpuscular volume alkaline phosphotase
alamine aminotransferase aspartate aminotransferase gamma-glutamyl transpeptidase half-pint equivalents of alcoholic
beverage drunk per day
The first five attributes are the results of blood tests thought to be related to liver functioning. The 345patientsareclassifiedintotwoclassesbythe severityoftheirlivermalfunctioning.Classtwois severe malfunctioning. In a random forests run,
 Fig. 5. Cluster averages—Bupa data.

the misclassification error rate is 28%. The variable importancegivenbyrandomforestsisinFigure4.
Blood tests 3 and 5 are the most important, fol- lowedbytest4.Randomforestsalsooutputsan intrinsicsimilaritymeasurewhichcanbeusedto cluster. When this was applied, two clusters were discovered in class two. The average of each variable is computed and plotted in each of these clusters in Figure 5.
An interesting facet emerges. The class two sub- jects consist of two distinct groups: those that have high scores on blood tests 3, 4, and 5 and those that have low scores on those tests.
11.3 Example III: Microarray Data
Randomforestswasrunonamicroarraylym- phomadatasetwiththreeclasses,samplesizeof 81and4,682variables(genes)withoutanyvariable selection [for more information about this data set, see Dudoit, Fridlyand and Speed, (2000)]. The error rate was low. What was also interesting from a sci- entific viewpoint was an estimate of the importance of each of the 4,682 gene expressions.
ThegraphinFigure6wasproducedbyarun of random forests. This result is consistent with assessments of variable importance made using other algorithmic methods, but appears to have sharper detail.
11.4 Remarks about the Examples
The examples show that much information is available from an algorithmic model. Friedman
(1999) derives similar variable information from a differentwayofconstructingaforest.Thesimilar- ityisthattheyarebothbuiltaswaystogivelow predictive error.
There are 32 deaths and 123 survivors in the hep- atitis data set. Calling everyone a survivor gives a baseline error rate of 20.6%. Logistic regression low- ers this to 17.4%. It is not extracting much useful informationfromthedata,whichmayexplainits inabilitytofindtheimportantvariables.Itsweak- ness might have been unknown and the variable importances accepted at face value if its predictive accuracywasnotevaluated.
Random forests is also capable of discovering importantaspectsofthedatathatstandarddata modelscannotuncover.Thepotentiallyinteresting clustering of class two patients in Example II is an illustration. The standard procedure when fitting data models such as logistic regression is to delete variables; to quote from Diaconis and Efron (1983) again, “statistical experience suggests that it is unwise to fit a model that depends on 19 variables withonly155datapointsavailable.”Newermeth- ods in machine learning thrive on variables—the more the better. For instance, random forests does notoverfit.Itgivesexcellentaccuracyonthelym- phoma data set of Example III which has over 4,600 variables, with no variable deletion and is capable of extracting variable importance information from the data.
STATISTICAL MODELING: THE TWO CULTURES 213
 Fig. 6. Microarray variable importance.

214 L. BREIMAN
These examples illustrate the following points:
•Higherpredictiveaccuracyisassociatedwith more reliable information about the underlying data mechanism.Weakpredictiveaccuracycanleadto questionable conclusions.
• Algorithmic models can give better predictive accuracythandatamodels,andprovidebetterinfor- mation about the underlying mechanism.
12. FINAL REMARKS
The goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involvingdata.Tomakemypositionclear,Iamnot against data models per se. In some situations they arethemostappropriatewaytosolvetheproblem. But the emphasis needs to be on the problem and on the data.
Unfortunately, our field has a vested interest in data models, come hell or high water. For instance, see Dempster’s (1998) paper on modeling. His posi- tiononthe1990Censusadjustmentcontroversyis particularlyinteresting.Headmitsthathedoesn’t know much about the data or the details, but argues thattheproblemcanbesolvedbyastrongdose of modeling. That more modeling can make error- riddendataaccurateseemshighlyunlikelytome.
Terrabytes of data are pouring into computers frommanysources,bothscientific,andcommer- cial, and there is a need to analyze and understand the data. For instance, data is being generated atanawesomeratebytelescopesandradiotele- scopes scanning the skies. Images containing mil- lions of stellar objects are stored on tape or disk. Astronomers need automated ways to scan their data to find certain types of stellar objects or novel objects. This is a fascinating enterprise, and I doubt if data models are applicable. Yet I would enter this inmyledgerasastatisticalproblem.
The analysis of genetic data is one of the most challenging and interesting statistical problems around.Microarraydata,likethatanalyzedin Section 11.3 can lead to significant advances in understanding genetic effects. But the analysis of variable importance in Section 11.3 would be difficulttodoaccuratelyusingastochasticdata model.
Problems such as stellar recognition or analysis of gene expression data could be high adventure for statisticians.Butitrequiresthattheyfocusonsolv- ing the problem instead of asking what data model theycancreate.Thebestsolutioncouldbeanalgo- rithmic model, or maybe a data model, or maybe a
combination. But the trick to being a scientist is to beopentousingawidevarietyoftools.
The roots of statistics, as in science, lie in work- ingwithdataandcheckingtheoryagainstdata.I hopeinthiscenturyourfieldwillreturntoitsroots. There are signs that this hope is not illusory. Over the last ten years, there has been a noticeable move toward statistical work on real world problems and reachingoutbystatisticianstowardcollaborative work with other disciplines. I believe this trend will continue and, in fact, has to continue if we are to survive as an energetic and creative field.

'''
# %%
def split_string(string, n):
    return [string[i:i+n] for i in range(0, len(string), n)]

string = "This is a long string"
n = len(stringg) // 4
chunks = split_string(stringg, n)

for chunk in chunks:
    print(chunk)
    print('\n')
    print('\n')
    print('\n')
    print('_____________________________________________________________________')
    print('\n')
    print('\n')
    print('\n')

# %%
